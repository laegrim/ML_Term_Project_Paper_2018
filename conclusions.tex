\section{Conclusions}

While we have had mixed results with some of our experiments its clear that blindly entering data to a feed-forward DNN will not give you good results for recommendation engines.

A deep matrix trained on just the reduced space of user item interactions can perform reasonably well even without the dimensionality of those spaces being optimized, and being provided no other contextual information about the user or movie.

\subsection{Experimental Discussion}
The results of our experiments with the neural network derived embedding layers feeding into a deep feedforward network were disheartening. While it is possible that simply training longer, or with a slightly different architecture, optimizer, different activations, or regularization would improve performance, these negative results likely mean that the features of the latent space learned  from the neural network derived embedding layers were not meaninful. Of course, this begs the question of what went wrong - but there's a larger lesson to be learned in how the quality of an embedding is measured.  In our case measuring the quality of semantic relationships on these embeddings had to be measured manually and TSNE was our best visuallization tool to see if similar movies or users had actually ended up clustered together in the latent space. Unfortuantely, it seems this is an unreliable way to asessing quality, and we may have told ourselves a just-so story. In the future, better methods must be developed or researched.

\subsection{Future Work}

There is still much to be done as limited time and a steep learning curve prevented us from accomplishing some of what we set out to do. It is clear that our Matrix factorization DNNs were able to learn something more from the representations we provided than the raw data. We would like to find some way to fine tune the number of features we select in the resulting embedding space to maximize its usefulness in the future. We may explore some method like GFM to accomplish this. We also need to work to find more useful neural network embeddings for our users and items.

While our work in this paper focused mainly on creating better representations for user movie interactions, there are lots of rich forms of data we have left unexplored that could help our model. providing information about genre, sentiment representation of descriptions, volume of ratings during the user movie interaction period (as a measure of popularity) could help provide models with significantly more information about the movies. Additional creating some representation for the viewing patterns of users could also enrich the model. These are all areas we have left untouched that each require extensive investigation. As we have learned in our experiments to date, models are very sensitive to how data is represented to them.

In addition to expanding our methods of creating better representations and providing richer context, we could also explore different network methods like using auto encoders. They could be used both as a replacement for our embedding layers, using the bottle neck layer, or having the auto encoder fill blacks in the reconstruction layer.